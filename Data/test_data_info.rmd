---
title: "R Notebook"
output: html_notebook
---
# Quick info
* For now we should use test_data.csv as a sanity check for our model
* Details of what pi, theta, and k should be is determined below
* There is a little section of a future goal Culpepper wanted us to do

# Load edmdata package.
This gives us the items_fractions data. Which consists of 536 subjects subject responses to J = 20 items.
Culpepper actually wrote a paper on this data which can be seen here https://link.springer.com/article/10.1007%2Fs11336-018-9643-8.

This is not the data we will test our models on. We use this to get parameters, then generate the test data from these parameters. I think its more likely we recover the parameters from the generated data than from the original data. So we use this initially to quickly verify that our models are working.

### Future goal?
Our final goal should be to do something similar as in the "other stuff" section.
1. run model on items_fractions to get parameter estimates
2. Generate N new datasets based our parameter estimates. Call these the monte carlo datasets
3. For each of the N monte carlo datasets, run these through the model and find monte carlo parameters
4. Finally we can use these monte carlo parameters to analyze how our model did.

```{r}
# Load edmdata package.
# This gives us the items_fractions dataframe
library(edmdata)
library(data.table)
dim(items_fractions)
set.seed(100)
```

```{r}
#find cluster sizes
k4 <- kmeans(items_fractions, centers = 4, nstart = 20)

# Get parameters
theta<-t(k4$centers)
pi_vec<-k4$size/nrow(items_fractions)
```


# Generating data from kmeans. aka our X matrix
This is the data we will use to initially test our models, our X matrix. It is 1000 subject responses to 20 items
```{r}
# This function generates data from the given parameters.
# theta -> parameter matrix
# pi_vec -> probability of being in a cluster
# seed -> random generator seed used for reproducibility
gen_data <- function(n,theta,pi_vec,seed){
  #set seed
  set.seed(seed)

  #generating data
  J<-nrow(theta)
  pi_vector <- pi_vec
  ngroups <- length(pi_vector)
  cluster_membership <- sample(1:ngroups,size=n,replace=T,prob=pi_vector)

  #create an empty matrix
  empty.matrix <- matrix(data = NA, nrow=n, ncol=J)
  #generating data
  for (i in 1:n) {
    for(j in 1:J){
      empty.matrix[i,j] <- rbinom(1, 1, prob=theta[j,cluster_membership[i]])
    }
  }

  # Turn data into dataframe
  column_names = paste("Question",seq(1,J,1))
  row_names = paste("Subject",seq(1,n,1))
  empty.matrix <- data.frame(empty.matrix,row.names=row_names)
  colnames(empty.matrix) <- column_names
  return(empty.matrix)
}
```
```{r}
n<- 1000 # 1000 simulated samples
df_simulated_data <- gen_data(n,theta,pi_vec,200)
write.csv(df_simulated_data,"test_data.csv")
```


# K vector
in other words which cluster each subject belongs to. This isn't relevant to our testing data.
```{r}
#finding centroids (theta) - p from bernoulli equation
#clusters = cutree(hclust(dist(items_fractions)), k=4) # get 4 clusters
clusters = k4$cluster
```

# Theta matrix
```{r}
row_names = paste("Cluster",seq(1,ncol(theta),1))
df_theta = data.frame(t(theta),row.names=row_names)
write.csv(df_theta,"test_theta_matrix.csv")
df_theta
```

# pi vector
```{r}
pi_vec
row_names = paste("Cluster",seq(1,ncol(theta),1))
df_pi_vec = data.frame(pi_vec,row.names=row_names)
colnames(df_pi_vec) = "Probability"
write.csv(df_pi_vec,"test_pi_vec.csv")
df_pi_vec
```

# K vector for testing data using k-means
I have no idea how well our k vector should line up with this since this uses a diffrent algorithm
```{r}
df_clusters = data.frame(kmeans(df_simulated_data, centers = 4, nstart = 20)$cluster)
colnames(df_clusters)="Class"
write.csv(df_clusters,"test_k_vector.csv")
df_clusters
```

# Other stuff
I didn't look at this other stuff too in depth. I was just interested in generating some testing data for us
```{r}
# function to find medoid in cluster i
# note: function not used in this code
clust.centroid = function(i, dat, clusters) {
  ind = (clusters == i)
  colMeans(dat[ind,])
}
```
```{r}

# This function generates data and then re runs kmeans algorithm
# to find the parameters based on the simulated.
kmeans_sim<-function(n,theta,pi_vec){
  #generating data
  J<-nrow(theta)
  pi_vector <- pi_vec
  ngroups <- length(pi_vector)
  cluster_membership <- sample(1:ngroups,size=n,replace=T,prob=pi_vector)

  #create an empty matrix
  empty.matrix <- matrix(data = NA, nrow=n, ncol=J)
  #generating data
  for (i in 1:n) {
    for(j in 1:J){
      empty.matrix[i,j] <- rbinom(1, 1, prob=theta[j,cluster_membership[i]])
    }
  }

  #Analyzing simulated data
  k4.sim <- kmeans(empty.matrix, centers = ngroups, nstart = 20)
  theta.sim<-t(k4.sim$centers)
  pi_vec.sim <- k4.sim$size/n

  #permute columns of theta.sim
  library(gtools)
  perm <- permutations(ngroups,ngroups,1:ngroups)

  #mean of the absolute differences
  mads<-numeric(nrow(perm))
  for (i in 1:nrow(perm)){
    #takes a possible column order to reorder theta.sim
    reordercol <- perm[i,]
    #theta.sim columns reordered
    theta.sim.new <- theta.sim[,reordercol]
    #find the absolute difference between theta and reordered theta.sim
    diftheta.new <- abs(theta - theta.sim.new)
    #find the mean of the absolute differences
    mads[i]<-mean(diftheta.new)
  }
  perm_indices<-which.min(mads)
  list(theta.sim[,perm[perm_indices,]],pi_vec.sim[perm[perm_indices,]])
}
```

```{r}
#number of variables = 20
#sample size = 1000
J <- 20  #number of items: 1-20
n<- 1000 #sampled 1000 times
kmeans_sim(n,theta,pi_vec)
```

```{r}
#number of variables = 20
#sample size = 1000
J <- 20  #number of items: 1-20
n<- 1000 #sampled 1000 times

R<-100
theta_output<-array(,dim=c(J,4,R))
pi_output<-matrix(,R,4)
for(r in 1:R){
  tmp<-kmeans_sim(n,theta,pi_vec)
  theta_output[,,r]<-tmp[[1]]
  pi_output[r,]<-tmp[[2]]
}
```

This finds bias, var, for the parameters and does some other stuff
```{r}
# bias for theta
mean_thetas<-apply(theta_output,c(1,2),mean)
bias_theta<-mean_thetas-theta
round(bias_theta,3)

#bias for pi_vec
mean_pi<-apply(pi_output,2,mean)
bias_pi<-mean_pi-pi_vec
round(bias_pi,3)


#compute variances
var_thetas<-apply(theta_output,c(1,2),var)
var_pi<-apply(pi_output,c(1,2),var)

#compute mse
mse_theta<-bias_theta^2+var_thetas
mse_pi<-bias_pi^2+var_pi

#root mean squared error
rmse_theta<-sqrt(mse_theta)
rmse_pi<-sqrt(mse_pi)

#average rmse
mean(rmse_theta)
mean(rmse_pi)

```
I wasn't able to get the rest of this code to run. So I just put eval=FALSE for the code chunk
```{r,eval=FALSE}

#--------------------------------
#simulation to evaluate selection of the number of clusters

library(NbClust)

kmeans_sim_inferclusters<-function(n,theta,pi_vec){
  #generating data
  J<-nrow(theta)
  pi_vector <- pi_vec
  ngroups <- length(pi_vector)
  cluster_membership <- sample(1:ngroups,size=n,replace=T,prob=pi_vector)

  #create an empty matrix
  empty.matrix <- matrix(0, nrow=n, ncol=J)
  #generating data
  for (i in 1:n) {
    for(j in 1:J){
      p1<-theta[j,cluster_membership[i]]
      p0<-1-p1
      empty.matrix[i,j] <- sample(0:1,size=1,replace=T,prob=c(p0,p1))
        # rbinom(1, 1, prob=theta[j,cluster_membership[i]])
    }
  }
  #Analyzing simulated data
  tmp_output<-NbClust(data=as.data.frame(empty.matrix), diss = NULL, distance = "euclidean",
          min.nc = 2, max.nc = 15, method = "kmeans",index="silhouette")
  # # index=c('KL','CH','Hartigan','CCC','Scott','Marriot','TrCovW','TraceW',
  #         # 'Friedman','Rubin','Cindex','DB','Silhouette','Duda','PseudoT2',
  #         # 'Ratkowsky','Ball','PtBiserial','Frey','McClain','Dunn',
  #         # 'SDindex','SDbw')
  # output<-list(tmp_output$Best.nc[1,],tmp_output$Best.nc[2,],apply(empty.matrix,2,mean))
  mean(empty.matrix1==empty.matrix)
  # output[[3]]
  empty.matrix1 <- empty.matrix
  rm(tmp_output)
}



R<-5
ns<-c(100,500,1000)
n<-100
ncluster_output<-matrix(NA,R,26)
index_output<-matrix(NA,R,26)
tmp_output<-matrix(NA,R,20)
for(r in 1:R){
  tmp<-kmeans_sim_inferclusters(n,theta,pi_vec)
  ncluster_output[r,]<-tmp[[1]]
  index_output[r,]<-tmp[[2]]
  tmp_output[r,]<-tmp[[3]]
}

prop_correct<-apply(ncluster_output==4,2,mean)
indices<-c('KL','CH','Hartigan','CCC','Scott','Marriot','TrCovW','TraceW',
        'Friedman','Rubin','Cindex','DB','Silhouette','Duda','PseudoT2',
        'Ratkowsky','Ball','PtBiserial','Frey','McClain','Dunn',
        'SDindex','SDbw')
indices[which(prop_correct>.5)]
```
